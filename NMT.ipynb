{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzgbpoPzFR0e"
      },
      "source": [
        "Preprocessing referenced from https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
        "\n",
        "Transformer from https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL6UZtvR8YMY",
        "outputId": "9437ed22-4c0b-4b0b-d3a6-c923a25e5f0f"
      },
      "source": [
        "!pip install -U torchtext==0.8.0\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torchtext==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_csqCCbH7gJo"
      },
      "source": [
        "import io\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "random.seed(26)\n",
        "np.random.seed(62)\n",
        "torch.manual_seed(297)\n",
        "\n",
        "device = 'cuda' #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdyA-tw17gJ2"
      },
      "source": [
        "# Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GE13vKt7gJ3"
      },
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "train_files = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_files = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_files = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRm9nqkD7gJ4"
      },
      "source": [
        "de_tokenizer = get_tokenizer('spacy', language='de')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "def build_vocab(file, tokenizer):\n",
        "    counter = Counter()\n",
        "    with io.open(file, encoding='utf8') as f:\n",
        "        for s in f:\n",
        "            counter.update(tokenizer(s))\n",
        "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "de_vocab = build_vocab(train_files[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_files[1], en_tokenizer)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcY1E9iU7gJ5"
      },
      "source": [
        "def transform_raw(vocab, tokenizer, raw_file):\n",
        "    string_iter = iter(io.open(raw_file, encoding='utf8'))\n",
        "    data = [torch.tensor([vocab[w] for w in tokenizer(s)]) for s in string_iter]\n",
        "    return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g64pir857gJ6"
      },
      "source": [
        "de_train = transform_raw(de_vocab, de_tokenizer, train_files[0])\n",
        "en_train = transform_raw(en_vocab, en_tokenizer, train_files[1])\n",
        "train = list(zip(de_train, en_train))\n",
        "\n",
        "de_val = transform_raw(de_vocab, de_tokenizer, val_files[0])\n",
        "en_val = transform_raw(en_vocab, en_tokenizer, val_files[1])\n",
        "val = list(zip(de_val, en_val))\n",
        "\n",
        "de_test = transform_raw(de_vocab, de_tokenizer, test_files[0])\n",
        "en_test = transform_raw(en_vocab, en_tokenizer, test_files[1])\n",
        "test = list(zip(de_test, en_test))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ0kg9w57gJ7"
      },
      "source": [
        "# Prepare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjUtWJyk7gJ7"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = en_vocab['<eos>']\n",
        "\n",
        "def preprocess_batch(batch):\n",
        "    de_batch, en_batch = [], []\n",
        "    for de_sentence, en_sentence in batch:\n",
        "        de_batch.append(torch.cat([\n",
        "            torch.tensor([BOS_IDX]), de_sentence, torch.tensor([EOS_IDX])\n",
        "            # de_sentence, torch.tensor([EOS_IDX])\n",
        "        ], dim=0))\n",
        "        en_batch.append(torch.cat([\n",
        "            torch.tensor([BOS_IDX]), en_sentence, torch.tensor([EOS_IDX])\n",
        "        ], dim=0))\n",
        "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    return (de_batch, en_batch)\n",
        "\n",
        "train_iter = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=preprocess_batch)\n",
        "val_iter = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=preprocess_batch)\n",
        "test_iter = DataLoader(test, batch_size=1, shuffle=False, collate_fn=preprocess_batch)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDcXBX3_7gJ8"
      },
      "source": [
        "# Define the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhlAzrp27gJ9"
      },
      "source": [
        "EN_VOCAB_SIZE = len(en_vocab)\n",
        "DE_VOCAB_SIZE = len(de_vocab)\n",
        "D_MODEL = 128\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.de_embed = nn.Embedding(DE_VOCAB_SIZE, D_MODEL)\n",
        "        self.en_embed = nn.Embedding(EN_VOCAB_SIZE, D_MODEL)\n",
        "        self.transformer = nn.Transformer(d_model=D_MODEL, \n",
        "            num_encoder_layers=2, num_decoder_layers=2, \n",
        "            dropout=0.5, dim_feedforward=2048)\n",
        "        self.fc1 = nn.Linear(D_MODEL, EN_VOCAB_SIZE)\n",
        "    \n",
        "    def forward(self, inputs, targets):\n",
        "        x = self.de_embed(inputs)\n",
        "        y = self.en_embed(targets)\n",
        "        tgt_mask = torch.triu(torch.ones(targets.size(0), targets.size(0)), diagonal=1).bool().to(device)\n",
        "        out = self.transformer(x, y, tgt_mask=tgt_mask)\n",
        "        out = self.fc1(out.permute(1, 0, 2)) # (batch, sequence, feature)\n",
        "        return out.permute(1, 0, 2).reshape(-1, EN_VOCAB_SIZE) # (sequence, batch, feature)\n",
        "\n",
        "net = Net().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29wY0lZ67gJ-"
      },
      "source": [
        "def to_sentence(ts):\n",
        "    \"\"\" Convert list of word-index to a sentence \"\"\"\n",
        "    return ' '.join([en_vocab.itos[x] for x in ts.squeeze() if x != PAD_IDX])\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def eval_model(max_output_len=50):\n",
        "    \"\"\" Run the NMT model on the validation set, return the average bleu-score \"\"\"\n",
        "    losses = 0.\n",
        "    scores = 0.\n",
        "    cnt = 0\n",
        "    net.eval()\n",
        "    for inputs_batch, targets_batch in val_iter:\n",
        "        for i in range(inputs_batch.size(1)):\n",
        "            inputs, targets = inputs_batch[:,i:i+1], targets_batch[:,i:i+1]\n",
        "            my_targets = targets[:1]\n",
        "            while len(my_targets) < max_output_len and my_targets[-1] != en_vocab['<eos>']:\n",
        "                pred = net(inputs.to(device), my_targets.to(device))\n",
        "                my_targets = torch.cat((\n",
        "                    my_targets, \n",
        "                    pred[-1,].argmax().unsqueeze(dim=0).unsqueeze(dim=0).to('cpu')\n",
        "                ))\n",
        "\n",
        "            target_sentence = to_sentence(targets[1:-1])\n",
        "            pred_sentence = to_sentence(my_targets[1:-1])\n",
        "            score = bleu_score([pred_sentence.split()], [[target_sentence.split()]])\n",
        "            scores += score\n",
        "            cnt += 1\n",
        "    \n",
        "    return scores/cnt\n",
        "\n",
        "def test_model():\n",
        "    \"\"\" Run the NMT model on the test set, show some example translation and average bleu-score \"\"\"\n",
        "    losses = 0.\n",
        "    scores = 0.\n",
        "    cnt = 0\n",
        "    net.eval()\n",
        "    for i, (inputs, targets) in enumerate(test_iter):\n",
        "        my_targets = targets[:1]\n",
        "        while len(my_targets) < 50 and my_targets[-1] != en_vocab['<eos>']:\n",
        "            pred = net(inputs.to(device), my_targets.to(device))\n",
        "            my_targets = torch.cat((\n",
        "                my_targets, \n",
        "                pred[-1,].argmax().unsqueeze(dim=0).unsqueeze(dim=0).to('cpu')\n",
        "            ))\n",
        "\n",
        "        target_sentence = to_sentence(targets[1:-1])\n",
        "        pred_sentence = to_sentence(my_targets[1:-1])\n",
        "        score = bleu_score([pred_sentence.split()], [[target_sentence.split()]])\n",
        "        scores += score\n",
        "        cnt += 1\n",
        "        if i < 10:\n",
        "            print(f'Bleu score: {score:.4f}')\n",
        "            print(f'Truth: {target_sentence} Pred: {pred_sentence}')\n",
        "    \n",
        "    print(f'Average Bleu score: {scores/cnt:.4f}')\n",
        "\n",
        "def evaluate():\n",
        "    \"\"\" Fast (not accurate) evaluation on validation set, return average loss \"\"\"\n",
        "    losses = 0.\n",
        "\n",
        "    net.eval()\n",
        "    for i, (inputs, targets) in enumerate(val_iter):\n",
        "        pred = net(inputs.to(device), targets[:-1,].to(device))\n",
        "\n",
        "        loss = criterion(pred.to('cpu'), targets[1:,].view(-1))\n",
        "        losses += loss.detach().item()\n",
        "    \n",
        "    return losses / (i+1)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYWRpk1JiFD5",
        "outputId": "73928cb4-5514-40f1-d3fd-a50d2e83a671"
      },
      "source": [
        "def train_network(epoch_range):\n",
        "    net.train()\n",
        "    for epoch in epoch_range:\n",
        "        losses = 0.\n",
        "        with tqdm(total=len(train_iter)) as pbar:\n",
        "            for i, (inputs, targets) in enumerate(train_iter):\n",
        "                optimizer.zero_grad()\n",
        "                pred = net(inputs.to(device), targets[:-1,].to(device))\n",
        "                loss = criterion(pred.to('cpu'), targets[1:,].view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                losses += loss.detach().item()\n",
        "                pbar.set_description(f'training loss: {losses/(i+1):.4f}')\n",
        "                pbar.update(1)\n",
        "\n",
        "        print(f'Epoch {epoch:2}, train loss: {(losses/(i+1)):.6f}, val loss: {evaluate():.6f}, val bleu-score: {eval_model():.4f}')\n",
        "\n",
        "train_network(range(1, 11))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training loss: 4.5861: 100%|██████████| 227/227 [01:24<00:00,  2.70it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  1, train loss: 4.586144, val loss: 3.656280, val bleu-score: 0.0142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 3.2244: 100%|██████████| 227/227 [01:24<00:00,  2.70it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  2, train loss: 3.224417, val loss: 2.928091, val bleu-score: 0.0739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.5937: 100%|██████████| 227/227 [01:24<00:00,  2.69it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  3, train loss: 2.593660, val loss: 2.527479, val bleu-score: 0.1236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2.1884: 100%|██████████| 227/227 [01:24<00:00,  2.69it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  4, train loss: 2.188400, val loss: 2.355101, val bleu-score: 0.1378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.8817: 100%|██████████| 227/227 [01:23<00:00,  2.70it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  5, train loss: 1.881680, val loss: 2.246559, val bleu-score: 0.1645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.6176: 100%|██████████| 227/227 [01:24<00:00,  2.68it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  6, train loss: 1.617633, val loss: 2.195639, val bleu-score: 0.1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.3845: 100%|██████████| 227/227 [01:24<00:00,  2.68it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  7, train loss: 1.384525, val loss: 2.233621, val bleu-score: 0.1734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.1656: 100%|██████████| 227/227 [01:24<00:00,  2.70it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  8, train loss: 1.165570, val loss: 2.266293, val bleu-score: 0.1757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.9717: 100%|██████████| 227/227 [01:23<00:00,  2.72it/s]\n",
            "  0%|          | 0/227 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  9, train loss: 0.971679, val loss: 2.341097, val bleu-score: 0.1732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.8041: 100%|██████████| 227/227 [01:24<00:00,  2.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10, train loss: 0.804086, val loss: 2.454007, val bleu-score: 0.1658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OGh21fLelrQ"
      },
      "source": [
        "# train_network(range(11, 16))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG6e3LeFnOMP",
        "outputId": "52633cfa-7899-4405-c6e5-61aed0a2a66e"
      },
      "source": [
        "test_model()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu score: 0.0000\n",
            "Truth: A man in an orange hat starring at something . \n",
            " Pred: A man with an orange hat , orange hat , orange hat , orange hat , orange hat is giving a beverage . \n",
            "\n",
            "Bleu score: 0.3986\n",
            "Truth: A Boston Terrier is running on lush green grass in front of a white fence . \n",
            " Pred: A male athlete runs across white grass ball in front of a white fence . \n",
            "\n",
            "Bleu score: 0.0000\n",
            "Truth: A girl in karate uniform breaking a stick with a front kick . \n",
            " Pred: A girl with a hand in a karate uniform . \n",
            "\n",
            "Bleu score: 0.3041\n",
            "Truth: Five people wearing winter jackets and helmets stand in the snow , with <unk> in the background . \n",
            " Pred: Five people in black and white hats stand in the snow outside in the background . \n",
            "\n",
            "Bleu score: 0.3247\n",
            "Truth: People are fixing the roof of a house . \n",
            " Pred: People on roof of a house on the house . \n",
            "\n",
            "Bleu score: 0.1288\n",
            "Truth: A man in light colored clothing photographs a group of men wearing dark suits and hats standing around a woman dressed in a <unk> gown . \n",
            " Pred: A man in dark - skinned bottoms , a group with a group of men and women and a group wearing dark - skinned men and dark - skinned woman in bright colors are standing around a group . \n",
            "\n",
            "Bleu score: 0.2438\n",
            "Truth: A group of people standing in front of an igloo . \n",
            " Pred: A group of people are standing outside of a hilly landscape . \n",
            "\n",
            "Bleu score: 0.3339\n",
            "Truth: A boy in a red uniform is attempting to avoid getting out at home plate , while the catcher in the blue uniform is attempting to catch him . \n",
            " Pred: A boy in a red jersey is trying to catch the base while the catcher in blue uniform is trying to catch him . \n",
            "\n",
            "Bleu score: 0.4889\n",
            "Truth: A guy works on a building . \n",
            " Pred: A guy working on a building . \n",
            "\n",
            "Bleu score: 0.6555\n",
            "Truth: A man in a vest is sitting in a chair and holding magazines . \n",
            " Pred: A man in a vest is sitting in a chair holding a shoe trophy . \n",
            "\n",
            "Average Bleu score: 0.1616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouh7LS7CjAVG"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}